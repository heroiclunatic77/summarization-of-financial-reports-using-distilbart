{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b150a4-cb1e-442e-b4f2-e139acf3b7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "373001d5-8da2-4c62-927a-fa62dd836225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import time\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from transformers import pipeline\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f851290-fd42-43af-9dff-2db5c08f080b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nirmitsachde/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK data files\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e4a22e-e832-4efa-b948-0f7b99e9fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SECFilingAnalyzer:\n",
    "    def __init__(self, cik):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Student pamperedrebel@gmail.com',\n",
    "            'Accept': 'application/json',\n",
    "            'Host': 'data.sec.gov'\n",
    "        }\n",
    "        self.cik = cik\n",
    "\n",
    "    def get_filing_links(self, num_years=5):\n",
    "        \"\"\"fetch 10-K filing links from SEC EDGAR db\"\"\"\n",
    "        time.sleep(0.1)\n",
    "        try:\n",
    "            url = f'https://data.sec.gov/submissions/CIK{self.cik}.json'\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            filings = {\n",
    "                'accessionNumber': data['filings']['recent']['accessionNumber'],\n",
    "                'reportDate': data['filings']['recent']['reportDate'],\n",
    "                'form': data['filings']['recent']['form']\n",
    "            }\n",
    "            k10_filings = [(date, f'https://www.sec.gov/Archives/edgar/data/{self.cik}/'\n",
    "                                 f'{acc.replace(\"-\", \"\")}/{acc}.txt')\n",
    "                           for date, acc, form in zip(filings['reportDate'], filings['accessionNumber'], filings['form'])\n",
    "                           if form == '10-K']\n",
    "            return k10_filings[:num_years]\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching SEC data: {str(e)}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6628509-978f-48e6-b7d7-b81327805e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentParser:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Student pamperedrebel@gmail.com',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9',\n",
    "            'Host': 'www.sec.gov'\n",
    "        }\n",
    "\n",
    "    def fetch_document(self, url):\n",
    "        \"\"\"fetch and parse SEC document\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, \"xml\")\n",
    "            return soup\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise ValueError(f\"Error fetching document: {e}\")\n",
    "\n",
    "    def extract_text_after_marker(self, soup, start_marker):\n",
    "        \"\"\"extract text after a specified marker\"\"\"\n",
    "        text_nodes = soup.get_text(separator=\"\\n\", strip=True)\n",
    "        start_index = text_nodes.find(start_marker)\n",
    "        if start_index != -1:\n",
    "            return text_nodes[start_index + len(start_marker):].strip()\n",
    "        return \"\"\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"cleaning and processing\"\"\"\n",
    "        # Remove URLs, XML tags, and unwanted patterns\n",
    "        patterns_to_remove = [\n",
    "            r\"http\\S+|www\\S+\",\n",
    "            r\"<.*?>\",\n",
    "            r\"\\bdei:.*?\\b\",\n",
    "            r\"\\becd:.*?\\b\",\n",
    "            r\"\\bus-gaap:.*?\\b\",\n",
    "            r\"Apple Inc\\. \\| \\d+ Form \\d+-K \\| \\d+\"\n",
    "        ]\n",
    "        for pattern in patterns_to_remove:\n",
    "            text = re.sub(pattern, '', text)\n",
    "\n",
    "        def roman_to_int(roman):\n",
    "            roman_values = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}\n",
    "            int_value = 0\n",
    "            prev_value = 0\n",
    "            for char in roman:\n",
    "                curr_value = roman_values.get(char, 0)\n",
    "                if curr_value > prev_value:\n",
    "                    int_value += curr_value - 2 * prev_value\n",
    "                else:\n",
    "                    int_value += curr_value\n",
    "                prev_value = curr_value\n",
    "            return int_value\n",
    "\n",
    "        text = re.sub(r'\\b[XIVLCDM]+\\b', lambda match: str(roman_to_int(match.group())), text)\n",
    "\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        cleaned_sentences = [sentence for sentence in sentences if len(sentence.split()) > 5]\n",
    "        return cleaned_sentences\n",
    "\n",
    "    def extract_tables_as_sentences(self, soup):\n",
    "        \"\"\"trying to convert table data to sentences\"\"\"\n",
    "        ix_elements = soup.find_all('ix:nonNumeric', recursive=True)\n",
    "        tabular_sentences = []\n",
    "        for ix_elem in ix_elements:\n",
    "            name = ix_elem.get('name', 'Unknown')\n",
    "            context = ix_elem.get('contextRef', 'Unknown')\n",
    "            value = ix_elem.get_text(strip=True)\n",
    "            if value and not re.match(r\"^[-\\d\\s]*$\", value):\n",
    "                sentence = f\"{name} (context: {context}) has a value of {value}.\"\n",
    "                tabular_sentences.append(sentence)\n",
    "        return tabular_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51ccae60-6935-464e-9560-c02bc5ee6e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text_traditional(text, sentence_count=2):\n",
    "    \"\"\"summarize text using LSA algorithm (a variant of TextRank)\"\"\"\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LsaSummarizer()\n",
    "    summary = summarizer(parser.document, sentence_count)\n",
    "    return ' '.join(str(sentence) for sentence in summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e8ec4e-d4c4-435a-9c68-a5f1265f5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text_modern(text_chunk, max_length=200):\n",
    "    \"\"\"summarize text using a distilbart model\"\"\"\n",
    "    device = 0 if torch.cuda.is_available() else -1 \n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=device, truncation=True)\n",
    "    summary_list = summarizer(text_chunk, max_length=max_length, min_length=int(max_length/2), do_sample=False)\n",
    "    return summary_list[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63409692-b81a-4da5-b310-2aae7a8a10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_large_text(text, chunk_size=500):\n",
    "    \"\"\"handle large texts by breaking them into smaller chunks with controlled output\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = [' '.join(sentences[i:i + chunk_size]) for i in range(0, len(sentences), chunk_size)]\n",
    "    summaries = [summarize_text_modern(chunk, max_length=200) for chunk in chunks]\n",
    "    return ' '.join(summaries[:3])  # Combine summaries of the first few chunks for a longer summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ccaa715-7208-4a05-b07d-8b80bb36d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_traditional(text, num_keywords=5):\n",
    "    \"\"\"extract single words using TF-IDF\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=num_keywords)\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    return vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb57010-9863-4e65-902d-608c632e9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_modern(text, num_keywords=5):\n",
    "    \"\"\"extract single words using KeyBERT\"\"\"\n",
    "    kw_model = KeyBERT()\n",
    "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words='english', top_n=num_keywords)\n",
    "    return [keyword[0] for keyword in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a7fcbfc-4a40-415e-8d81-1f78d8e4bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationMetrics:\n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def calculate_rouge_scores(self, reference, summary):\n",
    "        \"\"\"calculating ROUGE scores for summarization evaluation\"\"\"\n",
    "        scores = self.rouge_scorer.score(reference, summary)\n",
    "        return {\n",
    "            'rouge1_f1': scores['rouge1'].fmeasure,\n",
    "            'rouge2_f1': scores['rouge2'].fmeasure,\n",
    "            'rougeL_f1': scores['rougeL'].fmeasure\n",
    "        }\n",
    "\n",
    "    def calculate_semantic_similarity(self, reference, summary):\n",
    "        \"\"\"calculating semantic similarity using sentence embeddings\"\"\"\n",
    "        ref_embedding = self.sentence_model.encode([reference])[0]\n",
    "        sum_embedding = self.sentence_model.encode([summary])[0]\n",
    "        similarity = cosine_similarity([ref_embedding], [sum_embedding])[0][0]\n",
    "        return similarity\n",
    "\n",
    "    def evaluate_keywords(self, text, extracted_keywords):\n",
    "        \"\"\"evaluating keyword relevance using embedding similarity\"\"\"\n",
    "        text_embedding = self.sentence_model.encode([text])[0]\n",
    "        keyword_embeddings = self.sentence_model.encode(extracted_keywords)\n",
    "        similarities = cosine_similarity([text_embedding], keyword_embeddings)[0]\n",
    "        return {\n",
    "            'avg_relevance': np.mean(similarities),\n",
    "            'max_relevance': np.max(similarities),\n",
    "            'min_relevance': np.min(similarities)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f937f35-63e1-4b45-8140-b9db48790865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # defining CIKs and start markers for 3 chosen companies\n",
    "    companies = {\n",
    "        \"Apple\": {\"cik\": \"0000320193\", \"start_marker\": \"Unless otherwise stated, all information presented herein is based on the Companyâ€™s fiscal calendar\"},\n",
    "        \"Amazon\": {\"cik\": \"0001018724\", \"start_marker\": \"This Annual Report on Form 10-K and the documents incorporated herein by reference\"},\n",
    "        \"NVIDIA\": {\"cik\": \"0001045810\", \"start_marker\": \"Forward-Looking Statements\"}\n",
    "    }\n",
    "    \n",
    "    # html \n",
    "    html_content = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>SEC Filing Analysis</title>\n",
    "        <style>\n",
    "            table { width: 100%; border-collapse: collapse; }\n",
    "            th, td { border: 1px solid black; padding: 8px; text-align: left; vertical-align: top; width: 33.33%; }\n",
    "            th { background-color: #f2f2f2; }\n",
    "            td { max-height: 160px; overflow-y: auto; white-space: pre-wrap; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>SEC Filing Analysis Results</h1>\n",
    "        \n",
    "        <!-- Apple Section -->\n",
    "        <h2>Apple Inc.</h2>\n",
    "        <p><a href=\"apple 10k 2024.pdf\" target=\"_blank\">apple 10k 2024.pdf</a></p>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Type</th>\n",
    "                <th>Summary</th>\n",
    "                <th>Keywords</th>\n",
    "            </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apple 2024\n",
    "    apple_info = companies[\"Apple\"]\n",
    "    apple_analyzer = SECFilingAnalyzer(apple_info[\"cik\"])\n",
    "    apple_parser = DocumentParser()\n",
    "    \n",
    "    apple_links = apple_analyzer.get_filing_links(1)\n",
    "    \n",
    "    for date, link in apple_links:\n",
    "        try:\n",
    "            soup = apple_parser.fetch_document(link)\n",
    "            raw_text = apple_parser.extract_text_after_marker(soup, apple_info[\"start_marker\"])\n",
    "            \n",
    "            cleaned_text = ' '.join(apple_parser.clean_text(raw_text))\n",
    "            tabular_sentences = ' '.join(apple_parser.extract_tables_as_sentences(soup))\n",
    "            all_text = cleaned_text + ' ' + tabular_sentences\n",
    "            \n",
    "            traditional_summary = summarize_text_traditional(all_text, sentence_count=2)\n",
    "            traditional_keywords = extract_keywords_traditional(all_text, num_keywords=5)\n",
    "            \n",
    "            modern_summary = summarize_large_text(all_text, chunk_size=500)\n",
    "            modern_keywords = extract_keywords_modern(all_text, num_keywords=5)\n",
    "            \n",
    "            html_content += f\"\"\"\n",
    "                <tr>\n",
    "                    <td style=\"height:160px;\">Traditional</td>\n",
    "                    <td style=\"height:160px;\">{traditional_summary}</td>\n",
    "                    <td style=\"height:160px;\">{', '.join(traditional_keywords)}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"height:160px;\">Modern</td>\n",
    "                    <td style=\"height:160px;\">{modern_summary}</td>\n",
    "                    <td style=\"height:160px;\">{', '.join(modern_keywords)}</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Apple's {date} filing: {str(e)}\")\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "        </table>\n",
    "\n",
    "        <!-- Amazon and NVIDIA Section -->\n",
    "        <h2>Amazon and NVIDIA Comparison</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Company</th>\n",
    "                <th>Modern Summary</th>\n",
    "                <th>Modern Keywords</th>\n",
    "            </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Amazon and NVIDIA 2023\n",
    "    for company_name in [\"Amazon\", \"NVIDIA\"]:\n",
    "        info = companies[company_name]\n",
    "        analyzer = SECFilingAnalyzer(info[\"cik\"])\n",
    "        parser = DocumentParser()\n",
    "        \n",
    "        links = analyzer.get_filing_links(1)\n",
    "        \n",
    "        for date, link in links:\n",
    "            try:\n",
    "                soup = parser.fetch_document(link)\n",
    "                raw_text = parser.extract_text_after_marker(soup, info[\"start_marker\"])\n",
    "                \n",
    "                cleaned_text = ' '.join(parser.clean_text(raw_text))\n",
    "                tabular_sentences = ' '.join(parser.extract_tables_as_sentences(soup))\n",
    "                all_text = cleaned_text + ' ' + tabular_sentences\n",
    "                \n",
    "                modern_summary = summarize_large_text(all_text, chunk_size=500)\n",
    "                modern_keywords = extract_keywords_modern(all_text, num_keywords=5)\n",
    "                \n",
    "                html_content += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td style=\"height:160px;\">{company_name}</td>\n",
    "                        <td style=\"height:160px;\">{modern_summary}</td>\n",
    "                        <td style=\"height:160px;\">{', '.join(modern_keywords)}</td>\n",
    "                    </tr>\n",
    "                \"\"\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {company_name}'s {date} filing: {str(e)}\")\n",
    "\n",
    "    evaluator = EvaluationMetrics()\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for company_name, info in companies.items():\n",
    "        evaluation_results[company_name] = {\n",
    "            'traditional': {},\n",
    "            'modern': {}\n",
    "        }\n",
    "        \n",
    "        # ... (inside the processing loop for each company)\n",
    "        \n",
    "        # Add evaluation metrics\n",
    "        evaluation_results[company_name]['traditional'] = {\n",
    "            'rouge_scores': evaluator.calculate_rouge_scores(cleaned_text[:1000], traditional_summary),\n",
    "            'semantic_similarity': evaluator.calculate_semantic_similarity(cleaned_text[:1000], traditional_summary),\n",
    "            'keyword_metrics': evaluator.evaluate_keywords(cleaned_text, traditional_keywords)\n",
    "        }\n",
    "        \n",
    "        evaluation_results[company_name]['modern'] = {\n",
    "            'rouge_scores': evaluator.calculate_rouge_scores(cleaned_text[:1000], modern_summary),\n",
    "            'semantic_similarity': evaluator.calculate_semantic_similarity(cleaned_text[:1000], modern_summary),\n",
    "            'keyword_metrics': evaluator.evaluate_keywords(cleaned_text, modern_keywords)\n",
    "        }\n",
    "\n",
    "    # Add evaluation results to HTML output\n",
    "    html_content += \"\"\"\n",
    "    <h2>Evaluation Metrics</h2>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Company</th>\n",
    "            <th>Method</th>\n",
    "            <th>ROUGE-1</th>\n",
    "            <th>ROUGE-2</th>\n",
    "            <th>ROUGE-L</th>\n",
    "            <th>Semantic Similarity</th>\n",
    "            <th>Keyword Relevance</th>\n",
    "        </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    for company, results in evaluation_results.items():\n",
    "        for method, metrics in results.items():\n",
    "            html_content += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{company}</td>\n",
    "                <td>{method}</td>\n",
    "                <td>{metrics['rouge_scores']['rouge1_f1']:.3f}</td>\n",
    "                <td>{metrics['rouge_scores']['rouge2_f1']:.3f}</td>\n",
    "                <td>{metrics['rouge_scores']['rougeL_f1']:.3f}</td>\n",
    "                <td>{metrics['semantic_similarity']:.3f}</td>\n",
    "                <td>{metrics['keyword_metrics']['avg_relevance']:.3f}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "\n",
    "    html_content += \"</table>\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "        </table>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(\"sec_filing_analysis.html\", \"w\") as file:\n",
    "        file.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cd86674-7890-49d3-a706-b661633c5b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756e90d-6b3f-4ff4-86cd-e66a43a690b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
